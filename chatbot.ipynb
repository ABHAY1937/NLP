{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSEW31/NKC/zNidlxR9hH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ABHAY1937/NLP/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvIbkqOkdjh0",
        "outputId": "a69037b6-915e-4324-de09-fa2bd52c9d58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dnnNCpMtdM3Y"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMwC41Qudc2Z",
        "outputId": "3be94715-09cd-45c2-c945-656b4e5a1b2a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [*] abc................. Australian Broadcasting Commission 2006\n",
            "  [*] alpino.............. Alpino Dutch Treebank\n",
            "  [*] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [*] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [*] basque_grammars..... Grammars for Basque\n",
            "  [*] bcp47............... BCP-47 Language Tags\n",
            "  [*] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [*] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [*] book_grammars....... Grammars from NLTK Book\n",
            "  [*] brown............... Brown Corpus\n",
            "  [*] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [*] cess_cat............ CESS-CAT Treebank\n",
            "  [*] cess_esp............ CESS-ESP Treebank\n",
            "  [*] chat80.............. Chat-80 Data Files\n",
            "  [*] city_database....... City Database\n",
            "  [*] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [*] comparative_sentences Comparative Sentence Dataset\n",
            "  [*] comtrans............ ComTrans Corpus Sample\n",
            "  [*] conll2000........... CONLL 2000 Chunking Corpus\n",
            "Hit Enter to continue: \n",
            "  [*] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [*] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [*] crubadan............ Crubadan Corpus\n",
            "  [*] dependency_treebank. Dependency Parsed Treebank\n",
            "  [*] dolch............... Dolch Word List\n",
            "  [*] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [*] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [*] floresta............ Portuguese Treebank\n",
            "  [*] framenet_v15........ FrameNet 1.5\n",
            "  [*] framenet_v17........ FrameNet 1.7\n",
            "  [*] gazetteers.......... Gazeteer Lists\n",
            "  [*] genesis............. Genesis Corpus\n",
            "  [*] gutenberg........... Project Gutenberg Selections\n",
            "  [*] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [*] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [*] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [*] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [*] kimmo............... PC-KIMMO Data Files\n",
            "Hit Enter to continue: \n",
            "  [*] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [*] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [*] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [*] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [*] machado............. Machado de Assis -- Obra Completa\n",
            "  [*] masc_tagged......... MASC Tagged Corpus\n",
            "  [*] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [*] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [*] moses_sample........ Moses Sample Models\n",
            "  [*] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [*] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [*] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [*] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [*] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [*] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [*] nps_chat............ NPS Chat\n",
            "  [*] omw-1.4............. Open Multilingual Wordnet\n",
            "  [*] omw................. Open Multilingual Wordnet\n",
            "Hit Enter to continue: \n",
            "  [*] opinion_lexicon..... Opinion Lexicon\n",
            "  [*] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [*] paradigms........... Paradigm Corpus\n",
            "  [*] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [*] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [*] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [*] pl196x.............. Polish language of the XX century sixties\n",
            "  [*] porter_test......... Porter Stemmer Test Files\n",
            "  [*] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [*] problem_reports..... Problem Report Corpus\n",
            "  [*] product_reviews_1... Product Reviews (5 Products)\n",
            "  [*] product_reviews_2... Product Reviews (9 Products)\n",
            "  [*] propbank............ Proposition Bank Corpus 1.0\n",
            "  [*] pros_cons........... Pros and Cons\n",
            "  [*] ptb................. Penn Treebank\n",
            "  [*] punkt............... Punkt Tokenizer Models\n",
            "  [*] qc.................. Experimental Data for Question Classification\n",
            "  [*] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "Hit Enter to continue: \n",
            "  [*] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [*] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [*] sample_grammars..... Sample Grammars\n",
            "  [*] semcor.............. SemCor 3.0\n",
            "  [*] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [*] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [*] sentiwordnet........ SentiWordNet\n",
            "  [*] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [*] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [*] smultron............ SMULTRON Corpus Sample\n",
            "  [*] snowball_data....... Snowball Data\n",
            "  [*] spanish_grammars.... Grammars for Spanish\n",
            "  [*] state_union......... C-Span State of the Union Address Corpus\n",
            "  [*] stopwords........... Stopwords Corpus\n",
            "  [*] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [*] swadesh............. Swadesh Wordlists\n",
            "  [*] switchboard......... Switchboard Corpus Sample\n",
            "  [*] tagsets............. Help on Tagsets\n",
            "  [*] timit............... TIMIT Corpus Sample\n",
            "  [*] toolbox............. Toolbox Sample Files\n",
            "Hit Enter to continue: \n",
            "  [*] treebank............ Penn Treebank Sample\n",
            "  [*] twitter_samples..... Twitter Samples\n",
            "  [*] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [*] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [*] unicode_samples..... Unicode Samples\n",
            "  [*] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [*] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [*] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [*] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [*] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [*] webtext............. Web Text Corpus\n",
            "  [*] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [*] word2vec_sample..... Word2Vec Sample\n",
            "  [*] wordnet2021......... Open English Wordnet 2021\n",
            "  [*] wordnet2022......... Open English Wordnet 2022\n",
            "  [*] wordnet31........... Wordnet 3.1\n",
            "  [*] wordnet............. WordNet\n",
            "  [*] wordnet_ic.......... WordNet-InfoContent\n",
            "  [*] words............... Word Lists\n",
            "  [*] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "Hit Enter to continue: \n",
            "\n",
            "Collections:\n",
            "  [*] all-corpora......... All the corpora\n",
            "  [*] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [*] all................. All packages\n",
            "  [*] book................ Everything used in the NLTK Book\n",
            "  [*] popular............. Popular packages\n",
            "  [*] tests............... Packages for running tests\n",
            "  [*] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [*] abc................. Australian Broadcasting Commission 2006\n",
            "  [*] alpino.............. Alpino Dutch Treebank\n",
            "  [*] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [*] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [*] basque_grammars..... Grammars for Basque\n",
            "  [*] bcp47............... BCP-47 Language Tags\n",
            "  [*] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [*] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [*] book_grammars....... Grammars from NLTK Book\n",
            "  [*] brown............... Brown Corpus\n",
            "  [*] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [*] cess_cat............ CESS-CAT Treebank\n",
            "  [*] cess_esp............ CESS-ESP Treebank\n",
            "  [*] chat80.............. Chat-80 Data Files\n",
            "  [*] city_database....... City Database\n",
            "  [*] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [*] comparative_sentences Comparative Sentence Dataset\n",
            "  [*] comtrans............ ComTrans Corpus Sample\n",
            "  [*] conll2000........... CONLL 2000 Chunking Corpus\n",
            "Hit Enter to continue: \n",
            "  [*] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [*] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [*] crubadan............ Crubadan Corpus\n",
            "  [*] dependency_treebank. Dependency Parsed Treebank\n",
            "  [*] dolch............... Dolch Word List\n",
            "  [*] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [*] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [*] floresta............ Portuguese Treebank\n",
            "  [*] framenet_v15........ FrameNet 1.5\n",
            "  [*] framenet_v17........ FrameNet 1.7\n",
            "  [*] gazetteers.......... Gazeteer Lists\n",
            "  [*] genesis............. Genesis Corpus\n",
            "  [*] gutenberg........... Project Gutenberg Selections\n",
            "  [*] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [*] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [*] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [*] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [*] kimmo............... PC-KIMMO Data Files\n",
            "Hit Enter to continue: all\n",
            "  [*] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [*] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [*] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [*] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [*] machado............. Machado de Assis -- Obra Completa\n",
            "  [*] masc_tagged......... MASC Tagged Corpus\n",
            "  [*] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [*] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [*] moses_sample........ Moses Sample Models\n",
            "  [*] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [*] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [*] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [*] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [*] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [*] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [*] nps_chat............ NPS Chat\n",
            "  [*] omw-1.4............. Open Multilingual Wordnet\n",
            "  [*] omw................. Open Multilingual Wordnet\n",
            "Hit Enter to continue: \n",
            "  [*] opinion_lexicon..... Opinion Lexicon\n",
            "  [*] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [*] paradigms........... Paradigm Corpus\n",
            "  [*] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [*] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [*] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [*] pl196x.............. Polish language of the XX century sixties\n",
            "  [*] porter_test......... Porter Stemmer Test Files\n",
            "  [*] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [*] problem_reports..... Problem Report Corpus\n",
            "  [*] product_reviews_1... Product Reviews (5 Products)\n",
            "  [*] product_reviews_2... Product Reviews (9 Products)\n",
            "  [*] propbank............ Proposition Bank Corpus 1.0\n",
            "  [*] pros_cons........... Pros and Cons\n",
            "  [*] ptb................. Penn Treebank\n",
            "  [*] punkt............... Punkt Tokenizer Models\n",
            "  [*] qc.................. Experimental Data for Question Classification\n",
            "  [*] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "Hit Enter to continue: \n",
            "  [*] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [*] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [*] sample_grammars..... Sample Grammars\n",
            "  [*] semcor.............. SemCor 3.0\n",
            "  [*] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [*] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [*] sentiwordnet........ SentiWordNet\n",
            "  [*] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [*] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [*] smultron............ SMULTRON Corpus Sample\n",
            "  [*] snowball_data....... Snowball Data\n",
            "  [*] spanish_grammars.... Grammars for Spanish\n",
            "  [*] state_union......... C-Span State of the Union Address Corpus\n",
            "  [*] stopwords........... Stopwords Corpus\n",
            "  [*] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [*] swadesh............. Swadesh Wordlists\n",
            "  [*] switchboard......... Switchboard Corpus Sample\n",
            "  [*] tagsets............. Help on Tagsets\n",
            "  [*] timit............... TIMIT Corpus Sample\n",
            "  [*] toolbox............. Toolbox Sample Files\n",
            "Hit Enter to continue: \n",
            "  [*] treebank............ Penn Treebank Sample\n",
            "  [*] twitter_samples..... Twitter Samples\n",
            "  [*] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [*] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [*] unicode_samples..... Unicode Samples\n",
            "  [*] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [*] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [*] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [*] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [*] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [*] webtext............. Web Text Corpus\n",
            "  [*] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [*] word2vec_sample..... Word2Vec Sample\n",
            "  [*] wordnet2021......... Open English Wordnet 2021\n",
            "  [*] wordnet2022......... Open English Wordnet 2022\n",
            "  [*] wordnet31........... Wordnet 3.1\n",
            "  [*] wordnet............. WordNet\n",
            "  [*] wordnet_ic.......... WordNet-InfoContent\n",
            "  [*] words............... Word Lists\n",
            "  [*] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "Hit Enter to continue: \n",
            "\n",
            "Collections:\n",
            "  [*] all-corpora......... All the corpora\n",
            "  [*] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [*] all................. All packages\n",
            "  [*] book................ Everything used in the NLTK Book\n",
            "  [*] popular............. Popular packages\n",
            "  [*] tests............... Packages for running tests\n",
            "  [*] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "eOOslugXeh2F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "SxbiYhYOfs3D"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language is a fielfd of computer science, artificial intelligence\""
      ],
      "metadata": {
        "id": "0ilGFgAlfzgs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiAK29yDgBpY",
        "outputId": "28858271-5e33-48e5-8cf7-1d4e425c4f23"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural Language is a fielfd of computer science, artificial intelligence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgGIAAEXgGlh",
        "outputId": "b59891c9-812f-4ad2-e1f4-fd73d496c4c3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'is', 'a', 'fielfd', 'of', 'computer', 'science', ',', 'artificial', 'intelligence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "nsMfjhLxgQim"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"rocks :\",lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpors :\", lemmatizer.lemmatize(\"corpors\"))\n",
        "\n",
        "print(\"better : \", lemmatizer.lemmatize(\"better\",pos = \"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmEfnqVOgrgc",
        "outputId": "2d619916-4a2d-4b0f-fee0-0555e19a23b9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpors : corpors\n",
            "better :  good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"program\",\"programmer\",\"programming\",\"programmers\"]\n",
        "\n",
        "for w in words:\n",
        "  print(w, \" : \",ps.stem(w))\n",
        "\n",
        "demo =\"goods\"\n",
        "ps.stem(demo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "lMVFtAxLg4Uy",
        "outputId": "5670cef2-40d3-46ab-abf0-8c88053e3895"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program  :  program\n",
            "programmer  :  programm\n",
            "programming  :  program\n",
            "programmers  :  programm\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatterbot --index-url https://pypi.org/simple\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faxGY6r0qUyo",
        "outputId": "f6b75444-7c71-4a49-962d-2df002e4d7d0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chatterbot\n",
            "  Using cached ChatterBot-1.0.5-py2.py3-none-any.whl (67 kB)\n",
            "Collecting mathparse<0.2,>=0.1 (from chatterbot)\n",
            "  Using cached mathparse-0.1.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: nltk<4.0,>=3.2 in /usr/local/lib/python3.10/dist-packages (from chatterbot) (3.8.1)\n",
            "Collecting pint>=0.8.1 (from chatterbot)\n",
            "  Using cached Pint-0.22-py3-none-any.whl (294 kB)\n",
            "Collecting pymongo<4.0,>=3.3 (from chatterbot)\n",
            "  Using cached pymongo-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
            "Collecting python-dateutil<2.8,>=2.7 (from chatterbot)\n",
            "  Using cached python_dateutil-2.7.5-py2.py3-none-any.whl (225 kB)\n",
            "Collecting pyyaml<5.2,>=5.1 (from chatterbot)\n",
            "  Using cached PyYAML-5.1.2.tar.gz (265 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<2.2,>=2.1 (from chatterbot)\n",
            "  Using cached spacy-2.1.9.tar.gz (30.7 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatterbot_corpus #corpus : is working as a data set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3kwMHKbj7_J",
        "outputId": "2d4c84ed-9580-4f8b-bb4c-31c0db8c4dab"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: chatterbot_corpus in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: PyYAML<4.0,>=3.12 in /usr/local/lib/python3.10/dist-packages (from chatterbot_corpus) (3.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade chatterbot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO9LAFPykJ6y",
        "outputId": "9919711b-a1ea-4191-80e0-2bcce0f8537e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chatterbot\n",
            "  Using cached ChatterBot-1.0.5-py2.py3-none-any.whl (67 kB)\n",
            "Collecting mathparse<0.2,>=0.1 (from chatterbot)\n",
            "  Using cached mathparse-0.1.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: nltk<4.0,>=3.2 in /usr/local/lib/python3.10/dist-packages (from chatterbot) (3.8.1)\n",
            "Collecting pint>=0.8.1 (from chatterbot)\n",
            "  Using cached Pint-0.22-py3-none-any.whl (294 kB)\n",
            "Collecting pymongo<4.0,>=3.3 (from chatterbot)\n",
            "  Using cached pymongo-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
            "Collecting python-dateutil<2.8,>=2.7 (from chatterbot)\n",
            "  Using cached python_dateutil-2.7.5-py2.py3-none-any.whl (225 kB)\n",
            "Collecting pyyaml<5.2,>=5.1 (from chatterbot)\n",
            "  Using cached PyYAML-5.1.2.tar.gz (265 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<2.2,>=2.1 (from chatterbot)\n",
            "  Using cached spacy-2.1.9.tar.gz (30.7 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chatterbot import ChatBot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "qQfzr37boc_9",
        "outputId": "e736757d-a66f-4187-b630-3dadfae4c89a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-22d2a9e85e25>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatBot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chatterbot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chatterbot.trainers import ListTrainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "cS_07CAloqbr",
        "outputId": "1d584cc6-95f2-43c1-e71b-2f12fa6c3de6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-2858e68e6f06>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mListTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chatterbot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mnLMjONugWI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}